{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v6ax0kzXK8T"
      },
      "outputs": [],
      "source": [
        "# most code sourced from https://www.projectpro.io/article/python-chatbot-project-learn-to-build-a-chatbot-from-scratch/429\n",
        "# dataset created from scratch\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_root = '/content/drive/My Drive/emotionschatbot'\n",
        "\n",
        "import json\n",
        "import string\n",
        "import random\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "data_file = open(data_root + '/intents.json').read()\n",
        "data = json.loads(data_file)\n",
        "\n",
        "words = []\n",
        "classes = []\n",
        "data_x = []\n",
        "data_y = []\n",
        "\n",
        "for intent in data[\"intents\"]:\n",
        "  for pattern in intent[\"patterns\"]:\n",
        "    tokens = nltk.word_tokenize(pattern)\n",
        "    words.extend(tokens)\n",
        "    data_x.append(pattern)\n",
        "    data_y.append(intent[\"tag\"]),\n",
        "  if intent[\"tag\"] not in classes:\n",
        "    classes.append(intent[\"tag\"])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
        "words = sorted(set(words))\n",
        "classes = sorted(set(classes))\n",
        "\n",
        "training = []\n",
        "out_empty = [0] * len(classes)\n",
        "for idx, doc in enumerate(data_x):\n",
        "  bow = []\n",
        "  text = lemmatizer.lemmatize(doc.lower())\n",
        "  for word in words:\n",
        "    bow.append(1) if word in text else bow.append(0)\n",
        "  output_row = list(out_empty)\n",
        "  output_row[classes.index(data_y[idx])] = 1\n",
        "  training.append([bow, output_row])\n",
        "random.shuffle(training)\n",
        "train_x = np.array([np.array(x).flatten() for x in np.array(training)[:,0]])\n",
        "train_y = np.array([np.array(y).flatten() for y in np.array(training)[:,1]])\n",
        "x = tf.convert_to_tensor(train_x, dtype=tf.float32)\n",
        "y = tf.convert_to_tensor(train_y, dtype=tf.float32)\n",
        "\n",
        "from tensorflow.python.types.core import Tensor\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation=\"softmax\"))\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.01, epsilon=1e-6)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=[\"accuracy\"])\n",
        "print(model.summary())\n",
        "model.fit(x, y, epochs=150, verbose=1)\n",
        "\n",
        "def clean_text(text):\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  return tokens\n",
        "\n",
        "def bag_of_words(text, vocab):\n",
        "  tokens = clean_text(text)\n",
        "  bow = [0] * len(vocab)\n",
        "  for w in tokens:\n",
        "    for idx, word in enumerate(vocab):\n",
        "      if word == w:\n",
        "        bow[idx] = 1\n",
        "  return np.array(bow)\n",
        "\n",
        "def pred_class(text, vocab, labels):\n",
        "  bow = bag_of_words(text, vocab)\n",
        "  result = model.predict(np.array([bow]))[0]\n",
        "  thresh = 0.5\n",
        "  y_pred = [[indx, res] for indx , res in enumerate(result) if res > thresh]\n",
        "  y_pred.sort(key=lambda x: x[1], reverse=True)\n",
        "  return_list = []\n",
        "  for r in y_pred:\n",
        "    return_list.append(labels[r[0]])\n",
        "  return return_list\n",
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "  if len(intents_list) == 0:\n",
        "    result = \"Sorry! I do not understand.\"\n",
        "  else:\n",
        "    tag = intents_list[0]\n",
        "    list_of_intents = intents_json[\"intents\"]\n",
        "    for i in list_of_intents:\n",
        "      if i[\"tag\"] == tag:\n",
        "        result = random.choice(i[\"responses\"])\n",
        "        break\n",
        "  return result\n",
        "\n",
        "print('Hi! My name is CareBot, and I would love to hear about your day! If you have urgent needs, please respond with \"urgent\" and you will be redirected. To exit, please respond with\"0\".')\n",
        "while True:\n",
        "  message = input(\"\")\n",
        "  if message == \"0\":\n",
        "    break\n",
        "  if message is not None:\n",
        "    intents = pred_class(message, words, classes)\n",
        "    result = get_response(intents, data)\n",
        "    print(result)"
      ]
    }
  ]
}